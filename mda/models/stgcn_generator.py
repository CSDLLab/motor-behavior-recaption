import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib.transforms as mtransforms
from matplotlib.collections import PatchCollection

from models.backbone import st_gcn, DataBN
from models.base_model import BaseModel
from core.loss import sequence_diff
from utils.visualize import VisualizeCurve, VisualizeHeatmap


class STDiscriminator(nn.Module):
    """Discriminator for sequential motions

    """
    def __init__(self, graph, x_channels, u_channels, num_nodes, hidden_size, dropout=0.5, st_kernel=(3, 5), bn=False):
        super(STDiscriminator, self).__init__()
        self.x_channels = x_channels
        self.u_channels = u_channels
        self.num_nodes = num_nodes
        self.hidden_size = hidden_size
        self.dropout_rate = dropout
        self.st_kernel = st_kernel
        self.bn = bn

        self.adjacent_matrix = nn.Parameter(torch.from_numpy(graph), requires_grad=True)
        self.data_bn = DataBN(x_channels + u_channels, num_nodes)

        # GCN blocks
        self.layer0 = st_gcn(x_channels + u_channels, 64, st_kernel)
        self.layer1 = st_gcn(64, 128, st_kernel)
        self.layer2 = st_gcn(128, 256, st_kernel)
        self.layer3 = st_gcn(256, 512, st_kernel)

        # LSTM
        self.lstm1 = nn.LSTM(512 * num_nodes, 256, batch_first=True)
        self.lstm1_bn = nn.BatchNorm1d(256)
        self.lstm2 = nn.LSTM(256, 64, batch_first=True)

    def forward(self, x, u):
        self.lstm1.flatten_parameters()
        self.lstm2.flatten_parameters()

        xn, xc, xt, xv = x.size()
        un, uc, ut, uv = u.size()
        s = torch.cat([x, u], dim=1)
        s = self.data_bn(s)

        s, adj = self.layer0(s, self.adjacent_matrix)
        s, adj = self.layer1(s, adj)
        s, adj = self.layer2(s, adj)
        s, adj = self.layer3(s, adj)

        # reshape to 3D array of shape [batch_size, T, num_flatten_features]
        s = s.view(xn, xt, 512 * xv)

        s, _ = self.lstm1(s)
        s = s.permute(0, 2, 1)
        if self.bn:
            s = self.lstm1_bn(s)
        s = s.permute(0, 2, 1)
        s, _ = self.lstm2(s)
        return s

    def get_parameters(self):
        param_list = [
            {'params': self.parameters(), 'lr_mult': 1, 'decay_mult': 1}
        ]
        return param_list


class STGenerator(nn.Module):
    """Generator of spatial temporal models

    TODO: The initial state of inputs handling
    """
    def __init__(self, graph, z_channels=5, u_channels=1, x_channels=5, num_nodes=34, hidden_size=256, st_kernel=(3, 1), bn=False):
        """Generator of spatial temporal models

        :param graph: The graph associated with the nodes relationships
        :type graph: object
        :param in_channels: Number of channels for each node, defaults to 5
        :type in_channels: int, optional
        :param hidden_size: Number of dimensions for fusion operation in latent space, defaults to 256
        :type hidden_size: int, optional
        :param bn: If ``True`` use batch norm, defaults to False
        :type bn: bool, optional
        """
        super(STGenerator, self).__init__()
        self.graph = graph

        # TODO: This adjacent matrix must be generated by the ground truth graph structure
        self.adjacent_matrix = nn.Parameter(torch.from_numpy(graph), requires_grad=True)

        self.z_channels = z_channels
        self.u_channels = u_channels
        self.x_channels = x_channels
        self.num_nodes = num_nodes
        self.hidden_size = hidden_size
        self.bn = bn

        # self.gcn_encode0 = st_gcn(x_channels, x_channels * 2, kernel_size=st_kernel)
        # self.gcn_encode1 = st_gcn(x_channels * 2, x_channels, kernel_size=st_kernel)

        # Define the LSTM
        self.lstm1 = nn.LSTM((z_channels + u_channels + x_channels) * num_nodes, self.hidden_size, batch_first=True)
        self.lstm1_bn = nn.BatchNorm1d(self.hidden_size)
        self.lstm2 = nn.LSTM(self.hidden_size, self.hidden_size, batch_first=True)
        self.lstm2_bn = nn.BatchNorm1d(self.hidden_size)

        dense_layers = [nn.Linear(self.hidden_size, 512)]
        if self.bn:
            dense_layers.append(nn.BatchNorm1d(512))
        dense_layers.append(nn.LeakyReLU())

        dense_layers.append(nn.Linear(512, 32 * self.num_nodes))
        if self.bn:
            dense_layers.append(nn.BatchNorm1d(32 * self.num_nodes))
        dense_layers.append(nn.LeakyReLU())
        self.dense_net = nn.Sequential(*dense_layers)

        # GCN modules
        self.gcn0 = st_gcn(self.x_channels, self.x_channels, kernel_size=st_kernel)
        self.gcn1 = st_gcn(32, self.x_channels, kernel_size=st_kernel)
        self.bias_dense_net = nn.Linear(self.x_channels, self.x_channels)

    def forward(self, x0, z, u):
        self.lstm1.flatten_parameters()
        self.lstm2.flatten_parameters()

        xn, xc, xv = x0.size()
        zn, zc, zt, zv = z.size()
        un, uc, ut, uv = u.size()
        x = x0.unsqueeze(dim=2).repeat(1, 1, ut, 1)
        
        # STGCN
        # x, _ = self.gcn_encode0(x, self.adjacent_matrix)
        # x, _ = self.gcn_encode1(x, self.adjacent_matrix)

        inputs = torch.cat([x, u, z], 1)
        inputs = inputs.permute(0, 2, 3, 1).contiguous().view(un, ut, (xc + zc + uc) * uv)

        out, (ht1, ct1) = self.lstm1(inputs)
        out = out.permute(0, 2, 1)
        if self.bn:
            out = self.lstm1_bn(out)
        out = out.permute(0, 2, 1)

        out, (ht2, ct2) = self.lstm2(out, (ht1, ct1))
        out = out.permute(0, 2, 1)
        if self.bn:
            out = self.lstm2_bn(out)
        out = out.permute(0, 2, 1)
        out = self.dense_net(out)
        out = out.view(un, 32, ut, uv)

        out, new_adj = self.gcn1(out, self.adjacent_matrix)
        velocity, new_adj = self.gcn0(out, new_adj)
        velocity = velocity.permute(0, 3, 2, 1)
        velocity = self.bias_dense_net(velocity)
        velocity = velocity.permute(0, 3, 2, 1)
        pred_states = x + velocity
        return pred_states

    def get_parameters(self):
        param_list = [
            {'params': self.parameters(), 'lr_mult': 1, 'decay_mult': 1},
        ]
        return param_list


class STDSGenerator(STGenerator):
    """Single step dynamical system the next state only determined by last state

    :param x0: Initial states
    :type x0: torch.Tensor
    """
    def forward(self, x0, z, u):
        self.lstm1.flatten_parameters()
        self.lstm2.flatten_parameters()

        xn, xc, xv = x0.size()
        zn, zc, zt, zv = z.size()
        un, uc, ut, uv = u.size()
        inputs = torch.cat([u, z], 1)

        mixture = torch.cat([x0.unsqueeze(dim=2), inputs[:, :, :1, :]], dim=1)
        mixture = mixture.permute(0, 2, 3, 1).contiguous().view(un, 1, (xc + uc + zc) * uv)
        out, (ht1, ct1) = self.lstm1(mixture)
        out = out.permute(0, 2, 1)
        if self.bn:
            out = self.lstm1_bn(out)
        out = out.permute(0, 2, 1)

        out, (ht2, ct2) = self.lstm2(out)
        out = out.permute(0, 2, 1)
        if self.bn:
            out = self.lstm2_bn(out)
        out = out.permute(0, 2, 1)
        out = self.dense_net(out)
        out = out.view(un, 32, 1, uv)

        velocity, new_adj = self.gcn0(out, self.adjacent_matrix)
        next_states = x0.unsqueeze(dim=2) + velocity

        total_states = [next_states]
        for t in range(1, ut):
            mixture = torch.cat([next_states, inputs[:, :, t:t+1, :]], dim=1)
            mixture = mixture.permute(0, 2, 3, 1).contiguous().view(un, 1, (xc + uc + zc) * uv)
            out, (ht1, ct1) = self.lstm1(mixture, (ht1, ct1))
            out = out.permute(0, 2, 1)
            if self.bn:
                out = self.lstm1_bn(out)
            out = out.permute(0, 2, 1)

            out, (ht2, ct2) = self.lstm2(out, (ht2, ct2))
            out = out.permute(0, 2, 1)
            if self.bn:
                out = self.lstm2_bn(out)
            out = out.permute(0, 2, 1)
            out = self.dense_net(out)
            out = out.view(un, 32, ut, uv)

            out, new_adj = self.gcn1(out, self.adjacent_matrix)
            velocity, new_adj = self.gcn0(out, new_adj)
            next_states = next_states + velocity

            # upsampling sequence in latent space to euclidean space
            total_states.append(next_states)
        pred_seq = torch.cat(total_states, dim=2)
        return pred_seq


class STGCNGenerator(BaseModel):
    def __init__(self, cfg, graph) -> None:
        super(STGCNGenerator, self).__init__(cfg)
        self.cfg = cfg
        self.x_channels = cfg.DATASET.X_CHANNELS
        self.u_channels = cfg.DATASET.U_CHANNELS
        self.z_channels = cfg.DATASET.Z_CHANNELS
        self.num_nodes = cfg.DATASET.NUM_NODES
        self.time_steps = cfg.DATASET.TIME_STEPS
        self.st_kernel_g = cfg.MODEL.ST_KERNEL_G
        self.st_kernel_d = cfg.MODEL.ST_KERNEL_D
        self.end_epoch = cfg.TRAIN.END_EPOCH
        self.clip_value = cfg.TRAIN.CLIP_VALUE
        self.noise_level = cfg.TRAIN.NOISE_LEVEL
        self.max_num_show = cfg.TRAIN.MAX_NUM_SHOW
        self.lambda_recon = cfg.MODEL.LAMBDA_RECONSTRUCTION
        self.x_mean = np.asarray(cfg.DATASET.X_MEAN)
        self.x_std = np.asarray(cfg.DATASET.X_STD)
        self.u_mean = np.asarray(cfg.DATASET.U_MEAN)
        self.u_std = np.asarray(cfg.DATASET.U_STD)
        self.generator = STGenerator(graph,
                                     self.z_channels,
                                     self.u_channels,
                                     self.x_channels,
                                     self.num_nodes,
                                     st_kernel=self.st_kernel_g)
        self.discriminator = STDiscriminator(graph,
                                             self.x_channels,
                                             self.u_channels,
                                             self.num_nodes,
                                             hidden_size=256,
                                             st_kernel=self.st_kernel_d)
        self.lambda_angle = cfg.MODEL.LAMBDA_ANGLE
        self.optimizer_list = []
        self.log_dict = {}

    def cuda(self):
        self.generator = self.generator.cuda()
        self.discriminator = self.discriminator.cuda()
    
    def data_parallel(self):
        pass

    def set_optimizer(self, which_opt='RMSprop', lr=0.0001):
        if which_opt == 'Adam':
            self.opt_generator = optim.Adam(self.generator.get_parameters(), lr=lr, betas=(0.9, 0.99))
            self.optimizer_list.append(self.opt_generator)
            self.opt_discriminator = optim.Adam(self.discriminator.get_parameters(), lr=lr, betas=(0.9, 0.99))
            self.optimizer_list.append(self.opt_discriminator)
        elif which_opt == 'RMSprop':
            self.opt_generator = optim.RMSprop(self.generator.get_parameters(), lr=lr)
            self.optimizer_list.append(self.opt_generator)
            self.opt_discriminator = optim.RMSprop(self.discriminator.get_parameters(), lr=lr)
            self.optimizer_list.append(self.opt_discriminator)

    def save_checkpoint(self, epoch, filename):
        torch.save({
            'epoch': epoch,
            'state_dict': {
                'generator': self.generator.state_dict(),
                'discriminator': self.discriminator.state_dict()
            }
        }, filename)
    
    def load_checkpoint(self, filename):
        ckpt = torch.load(filename)
        # print("Looad checkpoint from {}".format(filename))
        self.generator.load_state_dict(ckpt['state_dict']['generator'])
        self.discriminator.load_state_dict(ckpt['state_dict']['discriminator'])

    def train(self, epoch, train_loader, writer_dict):
        self.generator.train()
        self.discriminator.train()

        for step, (x0, x, u) in enumerate(train_loader):
            self.reset_grad()
            x0 = x0.cuda()
            x = x.cuda()
            u = u.cuda()
            z = torch.rand(self.z_channels, 
                           self.time_steps, 
                           self.num_nodes, 
                           device=x.device).repeat(x.shape[0], 1, 1, 1) * self.noise_level

            # Train Discriminator
            if step % 1 == 0:
                if self.cfg.MODEL.NAME == 'STEncoderDynamicSystem':
                    fake_samples = self.generator(x0, z, u, x).detach()
                else:
                    fake_samples = self.generator(x0, z, u).detach()

                diff_fake = torch.cat([(fake_samples[:, :, 0] - x0).unsqueeze(2),
                                       fake_samples[:, :, 1:] - fake_samples[:, :, :-1]], dim=2)
                diff_real = torch.cat([(x[:, :, 0] - x0).unsqueeze(2),
                                       x[:, :, 1:] - x[:, :, :-1]], dim=2)
                seq_fake = self.discriminator(diff_fake, u)
                seq_real = self.discriminator(diff_real, u)

                loss_d = -torch.mean(seq_real) + torch.mean(seq_fake)

                writer_dict['writer'].add_scalar('train/loss_discriminator', loss_d,
                                                 global_step=writer_dict['train_global_step'])

                with torch.autograd.set_detect_anomaly(True):
                    loss_d.backward()
                    self.group_step([self.opt_discriminator])

                # clip weights of discriminator
                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), self.clip_value)

            # Train Gnerator
            if step % 1 == 0:
                if self.cfg.MODEL.NAME == 'STEncoderDynamicSystem':
                    fake_samples = self.generator(x0, z, u, x)
                else:
                    fake_samples = self.generator(x0, z, u)
                seq_fake = self.discriminator(fake_samples, u)
                loss_reconstruction = sequence_diff(fake_samples, x)
                loss_g = -torch.mean(seq_fake) + loss_reconstruction * self.lambda_recon

                writer_dict['writer'].add_scalar('train/loss_generator', loss_g,
                                                 global_step=writer_dict['train_global_step'])

                loss_g.backward()
                self.group_step([self.opt_generator])

                # clip weights of generator
                torch.nn.utils.clip_grad_norm_(self.generator.parameters(), self.clip_value)

            writer_dict['train_global_step'] += 1
            
        # The computation expensive visualization
        # if epoch % self.cfg.PRINT_FREQ == 0:
            # add muscle relationships counting
            # vis_g = VisualizeHeatmap(self.generator.adjacent_matrix.data.detach().cpu().numpy())
            # writer_dict['writer'].add_figure('train/generator', vis_g.show_heatmap(),
                                            #  global_step=writer_dict['train_global_step'])
            # vis_d = VisualizeHeatmap(self.discriminator.adjacent_matrix.data.detach().cpu().numpy())
            # writer_dict['writer'].add_figure('train/discriminator', vis_d.show_heatmap(),
                                            #  global_step=writer_dict['train_global_step']) 

            # if self.cfg.DATASET.DATASET == 'Muscle2Calcium':
                # vis = VisualizeCurve(fake_samples[0, 0].permute(1, 0).detach().cpu().numpy(),
                                    # x[0, 0].permute(1, 0).detach().cpu().numpy())
                # writer_dict['writer'].add_histogram('train/diff', fake_samples - x,
                                                    # global_step=writer_dict['train_global_step'])
                # writer_dict['writer'].add_figure('train/generated', vis.show_curve(),
                                                # global_step=writer_dict['train_global_step'])
            # elif self.cfg.DATASET.DATASET == 'Calcium2Muscle':
                # fig = self.visualize_tensor(torch.cat([x0.unsqueeze(dim=2), fake_samples], dim=2),
                                            # torch.cat([x0.unsqueeze(dim=2), x], dim=2))
                # writer_dict['writer'].add_figure('train/generated', fig,
                                                # global_step=writer_dict['train_global_step'])

    def eval(self, epoch, val_loader, writer_dict):
        """Evaluate the model performace with loss value and visualization

        :param epoch: [description]
        :type epoch: [type]
        :param val_loader: [description]
        :type val_loader: [type]
        :param writer_dict: [description]
        :type writer_dict: [type]
        """
        self.generator.eval()
        generated_samples = []
        with torch.no_grad():
            for step, (x0, x, u) in enumerate(val_loader):
                if step % self.time_steps == 0:
                    x0 = x0.cuda()
                    x = x.cuda()
                    u = u.cuda()
                    z = torch.rand(
                        self.z_channels, 
                        self.time_steps, 
                        self.num_nodes, 
                        device=x.device).repeat(x.shape[0], 1, 1, 1) * self.noise_level
                    fake_samples = self.generator(x0, z, u)
                    generated_samples.append(fake_samples)
            generated_seq = torch.cat(generated_samples, dim=2)
            generated_seq = generated_seq.detach().cpu().numpy() * self.x_std[None, :, None, None] + self.x_mean[None, :, None, None]
        return generated_seq
    
    def predict(self, x0, u):
        """Predict the succeeds behavoirs sequence with input x0 shape and control sequence

        :param x0: The initial muscle states [V, C] where V is number of nodes, C is number of channels of each node
        :type x0: torch.Tensor
        :param u: The control sequence
        :type u: torch.Tensor
        """
        self.generator.eval()
        with torch.no_grad():
            x0 = x0.cuda()
            u = u.cuda()

            # transfer to [N, C, T V] format
            x0 = x0.unsqueeze(0)
            u = u.unsqueeze(0)
            z = torch.zeros(
                self.z_channels, 
                u.size(2), 
                self.num_nodes, 
                device=u.device).repeat(u.shape[0], 1, 1, 1)
            pred_sequence = self.generator(x0, z, u).detach().cpu()
        return pred_sequence.squeeze(0)
    
    def visualize_tensor(self, tensor, tensor_real):
        max_num_show = min(self.max_num_show, tensor.size(0))
        tensor = tensor.permute(0, 2, 3, 1)[:max_num_show]
        tensor = tensor.detach().cpu().numpy() * self.x_std[np.newaxis, np.newaxis, np.newaxis, :] + self.x_mean[np.newaxis, np.newaxis, np.newaxis, :]
        tensor_real = tensor_real.permute(0, 2, 3, 1)[:max_num_show]
        tensor_real = tensor_real.detach().cpu().numpy() * self.x_std[np.newaxis, np.newaxis, np.newaxis, :] + self.x_mean[np.newaxis, np.newaxis, np.newaxis, :]

        batch_size = tensor.shape[0]
        time_steps = tensor.shape[1]
        fig, axes = plt.subplots(batch_size * 2, time_steps,
                                 figsize=(2*time_steps, 4*batch_size))
        xlim = [tensor[..., 3].min(), tensor[..., 3].max()]
        ylim = [tensor[..., 4].min(), tensor[..., 4].max()]

        for i, (seq, seq_real) in enumerate(zip(tensor, tensor_real)):
            axes[i*2, 0].set_ylabel('Generated')
            axes[i*2 + 1, 0].set_ylabel('Real')
            for t, (frame, frame_real) in enumerate(zip(seq, seq_real)):
                patches = []
                real_patches = []
                for rect in frame:
                    affine_transform = mtransforms.Affine2D()
                    affine_transform.rotate_deg_around(x=rect[3], y=rect[4], degrees=rect[2])
                    xy = (rect[3] - rect[0]/2, rect[4] - rect[1]/2)
                    width = rect[0]
                    height = rect[1]
                    patches.append(mpatches.Rectangle(xy, width, height, transform=affine_transform))
                plot_collection = PatchCollection(patches, alpha=0.4)
                colors = 0.1 * np.arange(len(patches))
                plot_collection.set_array(colors)
                axes[i*2, t].add_collection(plot_collection)
                axes[i*2, t].set_title('State-{}'.format(t))
                axes[i*2, t].set_xlim(xlim)
                axes[i*2, t].set_ylim(ylim)
                axes[i*2, t].set_xticks([])
                axes[i*2, t].set_yticks([])
                axes[i*2, t].axis('equal')

                for rect in frame_real:
                    affine_transform = mtransforms.Affine2D()
                    affine_transform.rotate_deg_around(x=rect[3], y=rect[4], degrees=rect[2])
                    xy = (rect[3] - rect[0]/2, rect[4] - rect[1]/2)
                    width = rect[0]
                    height = rect[1]
                    real_patches.append(mpatches.Rectangle(xy, width, height, transform=affine_transform))
                plot_collection_real = PatchCollection(real_patches, alpha=0.4)
                colors = 0.1 * np.arange(len(real_patches))
                plot_collection_real.set_array(colors)

                axes[i*2 + 1, t].add_collection(plot_collection_real)
                axes[i*2 + 1, t].set_title('State-{}'.format(t))
                axes[i*2 + 1, t].set_xlim(xlim)
                axes[i*2 + 1, t].set_ylim(ylim)
                axes[i*2 + 1, t].set_xticks([])
                axes[i*2 + 1, t].set_yticks([])
                axes[i*2 + 1, t].axis('equal')
        plt.tight_layout()
        return fig
